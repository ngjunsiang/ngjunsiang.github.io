---
title: "AIFE 2025: Post-Conference Review"
categories: education
layout: post
date: 2025-10-20
description: "A review of the AIFE 2025 conference, thoughts and follow-up."
permalink: aife-2025
---

# AIFE 2025: A Post-Conference Review

The recent [AIFE 2025 conference](https://www.ntu.edu.sg/education/inspire/ai-for-education-singapore), took place in Nanyang Technological University, Singapore, from 18 to 20 Nov. This is the second run of the conference, which had its inaugural run from 4–5 Nov 2024. This year's theme, "Theme: Learning About, With, and Beyond AI", brought together educators, researchers, and technologists to discuss the evolving role of AI in education. This is the Singapore chapter; other similarly named conferences were held or are planned in other countries; Indonesia had one this year, with conferences planned next year in Japan, China, and Denmark.

Day 1 focused on "Learning About AI", with talks and panels discussing AI literacy and curriculum development, as well as introducing current capabilities of AI tools being developed for education. Day 2 shifted to "Learning With AI", showcasing practical applications of AI in classrooms, adaptive learning systems, and personalized education, and also panel discussions about ethics of AI use and philosophical discussions on the purpose and future of AI in education. Finally, Day 3 explored "Learning Beyond AI", looking at future trends, ethical considerations, and the broader societal impacts of AI in education.

I was sponsored by my workplace to attend this conference, and took some notes on the sessions I attended.

# Day 1

After the opening remarks, the first keynote was delivered by Prof. Joseph Sung, Provost of the Lee Kong Chian School of Medicine, Nanyang Technological University. Drawing from his experience in medicine, he explains that AI cannot embody human values, particularly the trust and empathy that doctor-patient relationships are built on. The efficacy of care depends on this relationship; patients are not just looking for a cure, but for care. He posits a picture of "AI as maggot therapy" -- where AI consumes only those portions of a physician's work that is _no longer human_, leaving the physician to focus on the human aspects of care. A similar parallel can be drawn to education.

The second keynote, delivered by Prof Simon See from NVIDIA AI Technology Center, was an overview of the capabilities and caveats of current generative AI (genAI) tools, with a focus on large language models (LLMs) such as GPT-4 and Gemini. Brief bullet points: intelligence as an emergent (and not quite fully understood) property of next-token predictors, hallucination as a core characteristic of LLMs that cannot be fully eliminated, reinforcement learning with human feedback (RLHF) as a key technique for aligning LLM outputs with human values, and agents as actors in the (digital) world. He highlighted a pitfall of chatbots in their current form as a misleading interface: users tend to anthropomorphize chatbots, subconsciously following "it talks like a smart person, therefore it must be intelligent" heuristics: "we believe the pattern, not the mechanism". Thus, the importance of experts as people who can reason from first principles correctly so as to solve the right problem in the first place is not diminished, but rather increased by AI tools. And therefore, we (or some of us) still need to learn so as to know when the AI is wrong.

In the panel discussion that followed, between Prof Simon See, Prof Song Jie, Prof Mutlu Cukurova, and Prof Ho Shen Yong, a number of interesting questions and points were raised. What do we need students to do if AI can do everything? The importance of developing judgement and taste has increased; students have to become more task-aware and learn to define problems instead of confining themselves to solving known problems. Prof Song Jie highlighted some challenges of industrial upgrading in China, where lack of infrastructure for sharing knowledge of leading-edge capability leads to duplication of effort and slower progress. Prof Mutlu Cukurova raised his concern of a dehumanised education: lonely students interacting with AI tutors, without human connection. Real concerns also exist about data and bias: the models carry biases from their training data. Should the training data reflect societal biases for accuracy, or should it be "debiased" for fairness (equity)? Furthermore, many EdTech platforms look to maximise efficacy and engagement, but are they confusing efficacy with in-depth learning? Can in-depth learning be measured, and how if so?

## Parallel Talks

Briefly summarising the parallel talks I attended:

Prof Peter Johnson (Imperial College London) presented [Lambda Feedback](https://www.lambdafeedback.com/), a platform for connecting AI microservices to quiz platforms. It allows educators to plug in different AI models to provide feedback on student answers, enabling more personalised and context-aware feedback.

Dr Mohd Arif Mohamed (NTU) presented a custom AI workflow he uses to generate feedback for students, with human review and input at each stage, so as to "scale up" his ability to give personalised feedback without sacrificing quality or empathy. In his personal model, meaningful feedback is contextualised to what he has mentioned in class. It is also relational, not a one-way street; the feedback happens in a shared space between teacher and learner, influencing how students see themselves as learners, and inviting them to seek deeper consultation with him on specific issues.

Dr Ho Jia Xuan (NTU) presented his work on using process telemetry to understand student writing processes. By tracking metrics such as number of edits, pause times, and revision patterns, the AI-enhanced platform he is developing can detect moments of confusion or anxiety during the writing process, enabling real-time interventions and more meaningful feedback that is aware of the student's socio-emotional experience and writing process, not just the final essay.

# Day 2



# Day 3



---

# EdTech Masterplan 2030

To begin this review with some context, the Ministry of Education unveiled its [EdTech Masterplan 2030](https://www.moe.gov.sg/education-in-sg/educational-technology-journey/edtech-masterplan). Not explicitly mentioned is a strong push towards AI integration in education. However this was formulated, in staff meetings this year I felt it as a strong directive to adopt generative AI (genAI) tools in teaching and learning, but with little actionable guidance around how to do so, and why.

The masterplan mentions 3 key enablers needed to enable implementation:

> Key enabler 1: Learning analytics and data
> Key enabler 2: EdTech infrastructure and solutions to support school processes and meet rapidly changing needs
> Key enabler 3: EdTech ecosystem

I've seen all three play out on [Student Learning Space (SLS)](https://www.moe.gov.sg/education-in-sg/student-learning-space), the national learning platform for Singapore schools. Its primary focus appears to be delivery of micro-lesson units for primary and seconday schools, with some AI features such as a feedback bot that provides feedback on simple responses. Suitability for higher education (K-12 and up) appears limited, in what little time I have spent exploring it.

It is through the lens of these three enablers that I will review the conference sessions I attended.

## Learning analytics and data

The learning analytics and data currently available through SLS primarily focuses on tracking student progress through lesson units, and response accuracy. In AIFE 2025, I saw many presentations exploring other forms of learning analytics, with many presenters recognising their limitations. They are going into process telemetry: looking at number of edits, rate of edits, pause times and duration, and other process-oriented metrics to better understand student learning not just as an outcome but *as a process*.

This level of telemetry enables more sophisticated analyses of the kind that usually requires an attentive teacher who has noticed for example that Bobby has been staring at the screen without typing for the past 5 minutes, or Tim has been going over the same paragraph multiple times without meaningful changes. It can detect confusion or anxiety as it happens / if it happened, regardless of the polish in the final product, and thus unlocks the possibility of real-time interventions, or a discussion on the socio-emotional experience the student underwent.

As presented, these are still cutting-edge research projects, not yet ready for wider deployment across various educational settings. The telemetry itself is not a new thing; online ad platforms have been doing this over a decade for optimising user engagement, but that capability does not yet exist in a way suitable for education.

If we want to give more meaningful, engaging feedback beyond the same kind of responses we already give on assignments, I can already do this for 1–3 students in a small group setting, but to do it for a class of 20 **I need these attention-scaling tools**.

## EdTech infrastructure and solutions

The "strong directive to use genAI tools in teaching and learning" I mentioned earlier manifests on the ground as frequent, strong reminders to explore the use of genAI tools in lesson planning, classroom activities, and assessments, but with little explanation of *why* or *how*. In practice that looks a lot like:

A: "Have you tried using ChatGPT to generate some quiz questions for your next lesson?"
B: "Yes, I uploaded some past year papers and prompted it to generate questions for <topic>."
A: "You uploaded *what*?"

or

A: "Have you tried using ChatGPT to improve student feedback?"
B: "Yes, I copy-paste their answers into ChatGPT and ask it to give feedback, and sometimes I also tell it more about the student so it gives more sensitive and empathetic feedback."
A: "You told it *what* about the student?"

While we are still undergoing trial periods with (limited versions of) Copilot and Gemini, which experientially feel like using ChatGPT with the same amount of copy-pasting involved, there are few other platforms or tools being offered to educators to meaningfully integrate genAI into their teaching practice. If I talk to 5 other educators, I hear about 5 different genAI tools being trialled; three different AI voice generators, two different AI video generators, five different summarizers, and even more custom chatbot platforms. Everyone is hunting not for the best tool or even the right tool, but mainly for a free tool; with any luck this tool remains free for the next 1–2 years before they sink and we scramble for another free tool (and what about all that data???), or the company finds product-market fit and starts charging money for it and we scramble for another free tool (and what about all that data???).

For daily drivers we're stuck with Pair Chat, a government-provided Claude/ChatGPT-like interface, and whatever AI features get integrated into SLS over time, as the only officially-sanctioned platforms for anything involving potentially sensitive student data.

Those are solutions, not infrastructure. Infrastructure would be a platform that allows educators to plug in different genAI models or tools as they see fit, and tools for building genAI-powered applications beyond chatbots and prompt pasting. Most importantly, a robust data governance framework with accompanying tools for handling student data securely and ethically, such as data anonymization, consent management, and audit trails. Without these, educators get laden with more checklists and cognitive load about what data can be used where, and whatever productivity gains we get are squandered away doing compliance work manually.

At the talks I attended, I heard no educators using existing platforms, only existing models. They have had to develop their own solutions, and they see infrastructure as the next question mark they have to tackle. Prof. Peter Johnson (Imperial College London) came to present [Lambda Feedback](https://github.com/lambda-feedback/evaluation-function-api), a platform for connecting AI micro-services to quiz platforms. He's looking for collaborators to work on a common API schema for the microservice, because one doesn't exist yet.

We can't wait for vendors or the edtech industry to build these for us. We have to build them ourselves, or at least co-develop them with vendors, because we know our needs best.

## EdTech ecosystem

Locally, the edtech ecosystem is still nascent. There are few local edtech startups, and even fewer focusing on AI in education. As interesting as some of these educational platforms and tools are, they often do not align with local educational goals, curricula, or cultural norms; reseachers cannot afford hosting these solutions for local schools, schools often don't have enough funding to pay for a vendor to implement these solutions, and even when they do, the solutions are often existing products with some adaptation. Without startups or local companies building solutions from the ground up for local needs, the ecosystem remains fragmented and underdeveloped.

It feels like a choice between building everything in-house, or adopting foreign solutions that may not fit well. There is little middle ground where local edtech companies can co-develop solutions with schools and researchers to create tools that are both effective and contextually relevant.

There is excellent work being done to build some sorely needed tools; I particularly enjoy and am inspired by [Chan Kuang Wen's extensive work](https://kwen1510.github.io/home/) building semantic search and mentoring students working in this area. But when I ask myself where I see EdTech in the next 5 years ... I think it's got to be more than a smarter search box, another chatbot with customisable prompts, or a more contextually aware quiz generator or feedback bot. As <check-citation>Dr Simon See</check-citation> put it, we need to move on from prompt engineering to context engineering. If we're not careful, instead of maggot therapy that eats away the non-human parts, we might end up with Frankenstein's monster, a patchwork of disconnected AI tools that don't quite work with each other, leaving the human to do the gluework of copy-pasting prompts, downloading and uploading documents, and sieving out personal data manually.

# EdTech and AI in the next 5 years

Prof. Peter Johnson of ICL is trying to collaborate on AI microservices; Dr Ho Jia Xuan is experimenting with telemetry tools for writing; more than a number of names are working on context-aware bots. We need more of these efforts, ideas, and projects; AI for learning and AI for production have different goals and therefore needs. In 10 minutes I came up with some missing pieces:
- UI components for building AI-infused apps
- SDKs for data handling and model selection
- Specs and schemas for data exchange and interoperability
- Evaluation and test suites for learning AI

The last and IMO most important piece is the platform: I can't yet imagine what a *universal* learning platform would look like, but I can imagine and ideate what might work for my college. More importantly, I need to pull together a team interested to explore, build, and test this platform; this work cannot and should not be done alone. If, as Dr Maira Pratschke puts it, "AI in learning is all about context, context, context", then *something* needs to be managing that context, and a cohesive context manager *is* going to end up being a platform for all of the above.

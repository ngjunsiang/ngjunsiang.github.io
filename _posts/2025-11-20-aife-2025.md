---
title: "AIFE 2025: Post-Conference Review"
categories: education
layout: post
date: 2025-10-20
description: "A review of the AIFE 2025 conference, thoughts and follow-up."
permalink: aife-2025
---

# AIFE 2025: A Post-Conference Review

The recent [AIFE 2025 conference](https://www.ntu.edu.sg/education/inspire/ai-for-education-singapore), took place in Nanyang Technological University, Singapore, from 18 to 20 Nov. This is the second run of the conference, which had its inaugural run from 4–5 Nov 2024. This year's theme, "Theme: Learning About, With, and Beyond AI", brought together educators, researchers, and technologists to discuss the evolving role of AI in education. This is the Singapore chapter; other similarly named conferences were held or are planned in other countries; Indonesia had one this year, with conferences planned next year in Japan, China, and Denmark.

Day 1 focused on "Learning About AI", with talks and panels discussing AI literacy and curriculum development, as well as introducing current capabilities of AI tools being developed for education. Day 2 shifted to "Learning With AI", showcasing practical applications of AI in classrooms, adaptive learning systems, and personalized education, and also panel discussions about ethics of AI use and philosophical discussions on the purpose and future of AI in education. Finally, Day 3 explored "Learning Beyond AI", looking at future trends, ethical considerations, and the broader societal impacts of AI in education.

I was sponsored by my workplace to attend this conference, and took some notes on the sessions I attended.

**Note:** I wrote this review with the help of AI, but it is not written by AI. Claude Sonnet 4.5 helped to look up and verify references, add citations, and generally ease the load of organizing and editing my scribbled notes into a more coherent narrative. Any inaccuracies and misrepresentations are my own.

This review is written in three parts:
1. Conference Summary
2. Personal Thoughts and Responses
3. My view on EdTech and AI in the next 5 years

# Part 1: Conference Summary

These are from my notes taken during the conference sessions I attended.

## Day 1: Learning About AI

### Opening Keynote: Prof. Joseph Sung, Dean of Lee Kong Chian School of Medicine, NTU

Prof. Sung opened the conference with a medical metaphor that reframed the role of AI in education. He introduced the concept of **"AI as maggot therapy"** (Sung, 2024), drawing from maggot debridement therapy—a medical practice where maggots selectively consume dead tissue while preserving living tissue. In this analogy, AI handles routine and algorithmic tasks that are "no longer human," allowing educators to focus on empathy, mentorship, and human connection.

### Keynote: Prof. Simon See, Chief AI Technology Officer, NVIDIA AI Technology Centre

Prof. See provided a technical foundation for understanding modern AI systems. He explained **emergent properties** in large language models (LLMs)—capabilities that appear unpredictably as models scale. Importantly, Prof. See clarified that while the underlying mechanism (next-token prediction) remains constant, emergent capabilities appear in task performance at scale.

He addressed **hallucinations in LLMs**, explaining that these are inherent to how the systems work. While techniques like Retrieval-Augmented Generation (RAG) and multi-agent verification can reduce hallucinations by 60–80%, complete elimination remains impossible.

Prof. See explained that AI systems are trained through **reinforcement learning**, the process by which AI models learn to align with human preferences and values.

He distinguished between traditional LLMs (reactive, responding to prompts) and **agentic AI**—systems that perceive, decide, and act autonomously, using tools and executing multi-step tasks proactively. Examples include ChatGPT with Code Interpreter and research agents like AutoGPT.

A critical insight for education emerged from Prof. See's discussion of human-computer interaction: **"We believe the pattern, not the mechanism."** Users anthropomorphize chatbots, applying a "person schema" to human-like text, following the heuristic "Sounds smart → Must be intelligent → Can be trusted." This makes expert judgment MORE important in an AI-mediated world—experts reason from first principles and can identify when AI solves the wrong problem, even if it does so convincingly.

### Panel Discussion 1: The Risks and Opportunities of AI for Teaching and Learning

**Panelists:** Prof. Simon See, Prof. Song Jie, Prof. Mutlu Cukurova, and Prof. Ho Shen Yong

The panel explored several interconnected themes around AI's impact on education. They emphasized the development of **judgment and taste**—aesthetic, critical, and practical—as essential capabilities in an AI-augmented world. The discussion highlighted a shift toward **task-aware learning**, moving from problem-solving to problem-finding and problem-framing.

A tension emerged around humanization: panelists expressed concern about dehumanized education, imagining lonely students with AI tutors replacing human interaction. However, others countered that AI could enable more human connection by handling routine tasks. On the question of bias, the panel understood this to be a tension to be managed rather than a problem to be solved.

The panel raised questions about efficacy versus deep learning: Does optimizing for engagement metrics optimize for understanding? Can we distinguish between learning that looks effective and learning that is effective?

### Parallel Session: Lambda Feedback – Prof. Peter Johnson, Imperial College London

Prof. Johnson presented [**Lambda Feedback**](https://www.lambdafeedback.com/), a microservice architecture for pedagogical judgments that addresses the problem of "pedagogy lock-in." He explained that when educators adopt a platform, they also adopt all of the platform's assumptions. It becomes difficult to unbundle specific features or frameworks, effectively stalling innovation when educators want to implement practices that don't fit the platform's embedded design.

The system employs modular services (math feedback, code feedback, writing feedback) that connect via standardized APIs. Lambda Feedback decouples feedback generation from the Learning Management System, allowing educators to plug in specialized feedback services as needed.

Prof. Johnson posed the question: What would a standardized API schema for educational feedback look like? He invited collaborators interested in developing interoperability standards.

### Parallel Session: Personal Feedback at Scale—Dr. Mohamed Arif Mohamed

Dr. Mohamed presented an OCR-LLM system for generating personalized feedback at scale. The workflow involves: scanning student work, matching against an instructor comment bank, generating drafts in the instructor's voice, and implementing human-in-the-loop review before delivery to students.

Drawing on feedback literature, Dr. Mohamed emphasized that effective feedback should be **contextual** (referencing class content), **relational** (operating in a shared teacher-learner space), and **personal** (conveying authentic voice) (Boud & Molloy, 2013). The system addresses a specific problem: the emergence of copy-paste ChatGPT feedback that students recognize as inauthentic and generic. By maintaining the instructor's voice and requiring human review, the system aims to preserve the relational dimension of feedback while scaling the instructor's capacity.

### Parallel Session: Process Telemetry in Learning – Dr. Ho Jia Xuan

Dr. Ho introduced process telemetry that records keystrokes, mouse movements, pauses, and edits with timestamps. This approach can infer cognitive states: long pauses suggest planning or confusion, burst-pause-burst patterns indicate fluent writing, repeated revisions signal struggle, and high deletion rates may indicate anxiety.

Dr. Ho's focus was on using telemetry to observe process rather than just product. For example, detecting repeated edits that do not improve the writing—such as a student who kept editing the first sentence of an abstract without noticeably improving its clarity. He noted that large chunks of copy-paste could indicate AI use. This represents an evolution from outcome analytics (grades, completion rates) to process analytics (how learning happens).

## Day 2: Learning With AI

### Keynote: Prof. Miao Chunyan, Vice-President (Innovation & Entrepreneurship), NTU

Prof. Miao introduced a framework of **four types of agentic AI in learning**, each grounded in learning theory:

1. **Teachable AI**—Based on Learning-by-Teaching theory (Martin, 1985), where students teach an AI agent that acts as a novice learner
2. **Curious AI**—Drawing on Berlyne's theory of curiosity (1960), incorporating optimal complexity, novelty seeking, information gaps, and the drive to reduce uncertainty
3. **Persuasive AI**—Built on the Elaboration Likelihood Model (Petty & Cacioppo, 1986), distinguishing between central routes (logical argument) and peripheral routes (emotional appeal)
4. **Remembrance AI**—Implementing the spacing effect (Ebbinghaus, 1885), spaced repetition algorithms, and techniques for building remote associations

Prof. Miao presented the **CLC Virtual Singapura** case study, a 3D virtual learning environment using affective teachable agents. Students teach AI agents (such as water molecules that need to learn about diffusion) by constructing concept maps. The research showed that affective teachable agents can improve student reflection and learning outcomes.

### Keynote: Prof. Mutlu Cukurova, UCL Knowledge Lab, University College London

Prof. Cukurova presented a framework for **teacher-AI teaming** with three modes of interaction (Cukurova, 2024):

**Mode 1: Replacement**—AI takes over entire tasks. Concerns include accuracy issues and deskilling of educators.

**Mode 2: Amplification**—AI enhances human capability while humans retain control. Prof. Cukurova emphasized that amplification requires **internalization**—understanding how the AI works, not just its outputs. This is analogous to understanding a car's mechanics, an electric screwdriver's torque settings, or a cooking appliance's heat distribution. Without internalization, users cannot reason from first principles about AI behavior, troubleshoot problems, or judge output quality. With internalization, users achieve true complementarity between human and AI capabilities.

**Mode 3: Augmentation**—Human-AI synergy creates emergent capabilities. This mode is aspirational and rarely achieved in current systems.

Prof. Cukurova noted that most systems aim for Mode 1 (replacement), but education should aim for Mode 2 (amplification with internalization) with eventual progression to Mode 3.

He introduced the concept of **cognitive atrophy**—the loss of cognitive skills from AI over-reliance. Mechanisms include skill decay ("use it or lose it"), learned helplessness, and reduced productive struggle. Examples include the calculator effect on mental math and GPS effects on spatial navigation. Prof. Cukurova posed a counter-question: Perhaps we should let some skills atrophy to focus cognitive resources on higher-order capabilities?

Prof. Cukurova discussed **praxical teaming**, where AI learns from teacher corrections over time, building a more accurate model of teacher preferences and pedagogical judgment. He noted that this requires existing task synergy and raised appropriate skepticism—corrections may not transfer to novel contexts.

Regarding **GenAI for lesson planning**, Prof. Cukurova cited specific data showing 31% time reduction (from 81.5 minutes to 56.2 minutes). He briefly mentioned that a shortcoming of these findings is that they don't address whether lesson quality improves—planning faster doesn't necessarily mean planning better.

### Panel Discussion: Emotions in Learning

**Chair:** Asst. Prof. Tanmay Sinha (NTU/NIE)  
**Panelists:** Dr. Yang Yang (NTU/NIE), Dr. Aishah Abdul Rahman (NTU/NIE), Dr. Lee Vwen Yen Alwyn (NTU/NIE)

The panel began with a profound assertion: **"All learning is emotional."** Panelists drew from multiple research streams:

**Early literacy and socio-emotional development** (ages 4–7), exploring how emotional states affect foundational skill acquisition.

**Emotion socialization through conversation**, distinguishing between internal state talk ("You seem frustrated") and behavior talk ("Stop doing that"). Research shows that internal state talk better supports emotional development (Eisenberg et al., 1998).

**Executive function**—The cognitive control processes of working memory, inhibition, and cognitive shifting (Diamond, 2013). Emotional dysregulation impairs executive function, which in turn impairs learning.

**Yerkes-Dodson Law**—The inverted U-curve relationship between arousal and performance (Yerkes & Dodson, 1908). Optimal performance occurs at moderate arousal levels. Too little arousal leads to boredom; too much leads to anxiety. This suggests that some frustration is actually beneficial for learning.

**Epistemic emotions**—Emotions specifically related to knowledge-building: curiosity, confusion, surprise, and "aha moments" (Pekrun & Stephens, 2010). These emotions signal the learning process itself.

The **Emosense project** achieved 65% accuracy for detecting enjoyment, but only 33% for boredom and confusion. Research showed negative emotions at task start, with positive emotions at task end if students succeeded—suggesting that emotional trajectories matter more than emotional states at any single moment.

Panelists discussed **productive struggle** through the lens of Self-Determination Theory (Deci & Ryan, 2000), which identifies three psychological needs: autonomy, competence, and relatedness. Teacher-learner trust enables risk-taking necessary for productive struggle.

They explored **uncertainty tolerance** across three dimensions: behavioral (acting wisely despite uncertainty, craving novelty), cognitive (accepting that uncertainty is inherent to learning), and affective (managing the discomfort of not knowing).

A critical question emerged: If AI removes all uncertainty from learning tasks, does learning still happen? Can AI scaffold productive struggle without eliminating the struggle that produces learning?

### Parallel Session: Performative Assessment—Dr. Ricky Chua

Dr. Chua presented a solution to AI-facilitated cheating in presentations using the **SAMR Model** (Puentedura, 2006)—a framework for technology integration with four levels: **Substitution**: Technology replaces a tool with no functional change—**Augmentation**: Technology replaces a tool with functional improvement—**Modification**: Technology enables significant task redesign—**Redefinition**: Technology enables previously inconceivable tasks

His performative assessment solution imposed strict constraints: maximum 5 words per slide, script allowed only in notes (not visible to audience), no script reading during presentation, and images as memory triggers rather than content carriers. This design decouples what AI can do (generate slides and scripts) from what is assessed (live presentation skill and conceptual understanding).

Dr. Chua reported unintended outcomes: a 3% drop in teaching evaluations (students found the format more demanding), but students reported feeling the assessment was easier to prepare for, allowed more creativity, and gave them more control over their performance. The format made it immediately obvious when students didn't understand their material—they couldn't hide behind well-crafted slides.

### Parallel Session: AI in Art and Design Education—Assoc. Prof. Lisa Winstanley

Assoc. Prof. Winstanley articulated concerns specific to creative disciplines: environmental cost of AI image generation, copyright violation in training data, algorithmic bias in outputs, homogenization of aesthetic ("AI slop"), and most critically—AI undermines the creative process itself.

She argued that in art and design education, **the struggle IS the learning**. The iterative process of ideation, failure, revision, and discovery cannot be separated from the outcome. When AI generates images instantly, students bypass the very process they're meant to master.

Assoc. Prof. Winstanley's recommendations included: teach transparency about AI's limitations and training data, assess process documentation rather than finished products, allow students to ethically refuse AI use when it conflicts with learning goals, and encourage slow design—valuing iteration and failure as essential to creative development.

During panel discussion, insights emerged: students often use AI to avoid engagement in mandatory modules outside their major; could AI serve as a provocateur to spark creative thinking rather than resolve it?; and the importance of scaffolding—students should master basics first, then use AI for higher-order creative exploration once foundational skills are secure.

## Day 3: Learning Beyond AI

### Keynote: Prof. Mairéad Pratschke, MIT STEP Lab

Prof. Pratschke presented findings from MIT's research titled **"Your Brain on ChatGPT"**. She emphasized a fundamental principle: learning activities should match learning outcomes—a concept known as **Constructive Alignment** (Biggs, 1996). 

The problem: If the intended learning outcome is "analyze arguments critically," but the learning activity is "get ChatGPT to analyze arguments," there's a profound misalignment. The student doesn't practice the skill they're supposed to develop. Prof. Pratschke critiqued industry responses to educational concerns, noting that companies add surface features (like "study mode" or "education plans") without understanding how learning actually works. These changes confuse interface modifications with pedagogical design.

She discussed **AI as presence**, noting that different forms create different relationship expectations: a chatbot is perceived as a tool, an avatar as representation, an agent as acting on one's behalf, a tutor as a teaching relationship, and a digital human as a peer or companion. The design implication: form should match function and align with learning theory.

Prof. Pratschke introduced **"reasoners"** or **"thinking machines"**—a new class of AI systems (2024–2025) that show reasoning steps explicitly, such as OpenAI's o1 and Anthropic's extended thinking models. These systems make reasoning visible and auditable, potentially modeling problem-solving processes for students. However, she cautioned that they may encourage mimicking the visible reasoning pattern over developing genuine understanding.

She presented the **memory paradox**: Declarative knowledge (knowing THAT—facts and information) versus procedural knowledge (knowing HOW—skills and procedures). AI can store infinite declarative knowledge, but procedural knowledge requires practice. If AI executes procedures, students don't develop procedural knowledge. Drawing on Schema Theory (Bartlett, 1932; Sweller, 1988), Prof. Pratschke explained that learning requires building mental schemas, schemas require storing key information in long-term memory, if AI stores information externally then schemas don't form, and without schemas one cannot develop expertise.

She highlighted a shift from **prompt engineering to context engineering**. In 2025, effective AI use requires managing the entire context window: system prompts, retrieved documents (RAG), conversation history, tool definitions, few-shot examples, and user data. This is more complex than crafting the right prompt—it requires understanding what context is needed, managing token limits, and ensuring quality and coherence across all context components.

Prof. Pratschke introduced a **Contextual Intelligence Framework** identifying three types of intelligence needed for effective AI in education:
1. **Computational intelligence**—Understanding AI capabilities and limitations
2. **Pedagogical intelligence**—Understanding how learning works
3. **Content intelligence**—Understanding subject matter deeply

All three are required. Most current solutions are strong on computational intelligence but weak on pedagogical and content intelligence.

She referenced **Karpathy's Software 3.0** framework: Software 1.0 uses explicit programming (if-then rules), Software 2.0 uses machine learning (examples → learned patterns), and Software 3.0 uses foundation models plus context (LLMs configured through prompts and context). This represents a shift from code to data to prompts/context as the primary interface for creating software behavior.

### Keynote: Prof. Phillip Dawson, Deakin University

Prof. Dawson presented on **assessment design for the AI era**, opening with a core principle: **"If AI can do your assessment task and pass, you have an assessment design problem"** (Dawson, 2024). This reframes the challenge from "How do we detect AI?" to "How do we design assessments that AI can't easily complete?"

He presented the **TEQSA framework** (Lodge et al., 2023)—Australia's Tertiary Education Quality and Standards Agency guidelines—with five propositions:

1. **Emphasize appropriate, authentic AI engagement**—Rather than banning AI, teach students when and how to use it appropriately
2. **Systemic program assessment**—Design assessment at the program level, not just individual assignment level
3. **Emphasize process**—Assess learning journeys, not just products
4. **Provide opportunities for collaboration**—Both human-human and human-AI collaboration
5. **Strategic security at meaningful points**—Apply resource-intensive security only at key progression/completion milestones

Prof. Dawson emphasized that validity should take precedence over cheating prevention. He introduced **future-authentic assessment**—if AI will be ubiquitous in future work contexts, then assessing students without AI doesn't prepare them for reality. We should assess AI-augmented performance, focusing on judgment, creativity, and synthesis skills.

He presented the concept of **reverse scaffolding**: Traditional scaffolding provides more support early and gradually reduces it; reverse scaffolding prohibits AI use early (when students are developing foundational skills) and allows AI use once skills are mastered. For example: Year 1 students write without AI, Year 2 students can use AI for drafting but must show revision processes, Year 3 students have full AI access but are assessed on judgment and quality of final work. This prevents skill atrophy while allowing efficiency gains.

Prof. Dawson revisited **Zone of Proximal Development (ZPD)** (Vygotsky, 1978)—tasks that students cannot complete alone but can complete with appropriate support. This is the "sweet spot" for learning. The problem with many AI tools: traditional scaffolding adjusts to learner level, but AI tools often jump directly to expert-level performance, bypassing the ZPD entirely. No learning happens when students skip from novice directly to expert output.

The solution: design AI tools that provide graduated support, maintaining students within their ZPD.

He explained **Cognitive Load Theory** (Sweller, 1988) with three types of cognitive load:
1. **Intrinsic load**—Inherent complexity of the material (KEEP—this is learning)
2. **Extraneous load**—Poor design and distractions (REMOVE—this wastes cognitive capacity)
3. **Germane load**—Processing that builds schemas (SUPPORT—this is learning)

AI should remove extraneous load and support germane load, but NOT remove intrinsic load. Example: AI formatting citations is appropriate (removes extraneous load); AI writing arguments is inappropriate (removes intrinsic load that constitutes the learning itself).

Prof. Dawson introduced **evaluative judgment** (Tai et al., 2018)—the capability to judge quality of work, both one's own and others'. This is essential for self-regulated learning and develops through practice. In AI contexts, students must judge AI outputs, decide what to keep or revise, and evaluate appropriateness of AI use for specific tasks. However, evaluative judgment alone is not sufficient—students need additional capabilities to work effectively with AI.

He distinguished between **discursive and structural assessment approaches**: **Discursive**: Instructions about AI use ("You may use AI for editing but not drafting"; "Cite all AI use")
- **Structural**: Task requirements that make certain behaviors unavoidable (oral defenses, live coding, iterative submissions showing process, portfolios with reflections)

Prof. Dawson expressed preference for structural approaches as they're harder to circumvent than discursive approaches, which rely on student compliance.

### Parallel Session: Digital Portfolio Implementation—NIE Team

The NIE team presented their work on digital portfolios aligned with Singapore's **TE²¹ Model** (Teacher Education for 21st Century)—NIE's framework guiding teacher education since approximately 2009.

The TE²¹ Model identifies three core competencies:
1. Professional Practice
2. Personal Growth and Development  
3. Leadership and Agency

And five teacher roles:
1. Shapers of Character
2. Creators of Knowledge
3. Facilitators of Learning
4. Architects of Learning Environments
5. Agents of Educational Change

The team distinguished three types of digital portfolios:

**Learning Portfolio**—Process and developmental focus, documents the learning journey, audience is self and teacher, content includes drafts/reflections/growth evidence, used for formative assessment

**Showcase Portfolio**—Presentation focus, displays accomplishments and best work, audience is external (employers, admissions), content is polished products, used for summative assessment

**Teaching Portfolio**—Professional focus, documents teaching practice and philosophy, audience is supervisors/tenure committees, content includes lesson plans/student work/teaching reflections, used for professional evaluation

The NIE implementation focuses on learning portfolios to support pre-service teacher development.

### Parallel Session: AI Reflection Assistant—NIE Research Team

The research team presented their **AI Reflection Assistant**, designed with principles aligned with **Self-Determination Theory** (Deci & Ryan, 2000):

**Autonomy support**—The AI encourages thinking rather than providing answers, prompting students to develop their own insights

**Competence support**—The AI develops students' reflective capability rather than doing the reflection work for them

The design is growth-focused: it avoids giving answers (which creates dependency) and instead asks questions that scaffold deeper reflection.

The team adapted **Gibbs' Reflective Cycle** into a four-level framework:
1. **What happened?** (Description)
2. **Why?** (Analysis & Interpretation)  
3. **So what?** (Meaning & Application)
4. **Now what?** (Implications for Action)

This condenses Gibbs' original six stages. Alternative models referenced include Rolfe et al. (2001) and Borton (1970) with their "What? So what? Now what?" structure.

The team described the **reflection depth hierarchy** (Hatton & Smith, 1995): **Descriptive** (surface)—Simply recounting what happened—**Dialogic** (exploring)—Questioning and exploring multiple perspectives—**Critical** (deep)—Examining underlying assumptions and power structures

The AI's role is to move students progressively through these levels.

**Preliminary findings** indicated that students find the AI prompting helpful without feeling intrusive, they value directional guidance over prescriptive answers, and the approach aligns with growth mindset development.

A critical design principle emerged: **human agency**. The learner must take an active role in interpreting and engaging with feedback. The AI provides prompts, directions, and frameworks, but the HUMAN does the thinking, makes connections, draws conclusions, and plans actions. This prevents copy-paste compliance, surface engagement, dependency, and loss of reflective capacity. The design principle: AI as catalyst, not provider.

### Parallel Session: Knowledge Building with AI – Dr. Lydia Cao (University of Toronto), Assoc. Prof. Bodong Chen (University of Pennsylvania), Dr. Teo Chew Lee (NTU/NIE), and Dr. Katherine Yuan (NTU/NIE)

This collaborative session explored integrating AI into Knowledge Building (KB) pedagogies. Dr. Chen opened by contrasting **Knowledge Building** with traditional knowledge transmission:

**Transmission model**: Teacher possesses knowledge → transmits to students → students receive and memorize → assessment measures acquisition

**Knowledge Building** (Bereiter & Scardamalia, 1993): Collaborative advancement of community knowledge, students as knowledge creators not just consumers, focus on idea improvement not just learning, collective responsibility for community's knowledge growth, modeled on how scientific communities actually work.

Historical context includes social constructivism (Vygotsky), communities of practice (Wenger), and Computer-Supported Collaborative Learning (CSCL). The technology platform is **Knowledge Forum** (originally CSILE).

The session covered the **12 Knowledge Building Principles** (Scardamalia, 2002), highlighting several in detail:

**Principle 5: Epistemic Agency**—Students as agents of their own knowledge advancement. They set goals, decide which problems to tackle, control the inquiry process, and take responsibility for community knowledge growth.

**Principle 11: Knowledge-Creating Dialogue**—Building on ideas, identifying contradictions, seeking evidence, reformulating theories, aiming for continuous improvement. Dialogue is tentative and exploratory; questions are valued; "I don't know" is a legitimate and productive position.

**Principle 12: Transformative Assessment**—Assess the state of community knowledge, identify the cutting edge of understanding, feed forward to next problems. Assessment transforms into new inquiry rather than terminating learning.

A **critical design question** emerged: How do we distribute epistemic agency between AI and students? Traditional KB gives students full epistemic agency. With AI integration, how much agency goes to AI versus students? This is a delicate balance—if AI suggests problems, it reduces student agency; if AI generates ideas, it reduces student ownership; if AI decides direction, it undermines KB principles.

**Dr. Lydia Cao** introduced the **Idea Constellation Map**—a visualization tool bridging the inward view (what OUR community understands) and the outward view (what the WORLD knows—frontier knowledge). The gap between these views is where the community can grow.

Features include: zooming in to see detail, uncovering hidden connections (AI finds links students might miss), and overlaying community knowledge against world knowledge. Example: A community studying climate change maps their collective understanding, then AI overlays this against the scientific frontier, revealing gaps and opportunities for knowledge advancement.

This represents sophisticated AI use—not giving answers or generating content, but providing perspective on community knowledge and scaffolding metacognition about collective understanding.

**Assessing collaborative knowledge building** emerged as an open challenge during the session. How do we assess community knowledge (not individual achievement), idea improvement (not mastery of fixed content), and collective advancement (not grades and ranking)? Traditional metrics fail here: individual tests measure personal knowledge, grades assume stable endpoints, ranking assumes competition. Potential approaches include semantic network analysis, KB discourse analysis, idea trajectory analysis, and "rise above" analysis. AI could help track idea evolution, map knowledge networks, identify emergent patterns, and visualize collective progress—but human judgment is still needed to determine what constitutes genuine "advancement."

The **KB Loop** provides a recursive cycle:
1. Questions/Problems → 
2. Create ideas & artifacts → 
3. Experiment/test → 
4. Share/build/connect → 
(back to 1)

All stages lead to better ideas. The cycle is continuous with no fixed endpoint—knowledge building is about ongoing improvement. AI's potential role at each stage: help identify gaps and generate questions, scaffold idea generation, support experimental design, and facilitate connection-making. But students must drive the cycle.

**Dr. Chen presented ChatGPT integration patterns**:

**Pattern 1—Explore Problem Space**: Students face a complex problem, ChatGPT generates multiple possible approaches, students evaluate and select among options. AI expands the possibility space; students maintain decision-making authority. Example: Problem of reducing school waste → ChatGPT suggests 10 different approaches → students evaluate feasibility and choose direction. Key principle: AI generates options, students exercise judgment.

**Pattern 2—Build Through Discourse**: Students propose an explanation, ChatGPT plays devil's advocate by asking probing questions and identifying weaknesses, students must defend or revise their thinking. Example: Student claims "Recycling solves the waste problem" → ChatGPT responds "What about contamination rates? What are the energy costs of recycling?" → student must address these challenges. Key principle: AI critiques, students improve ideas.

This is pedagogically sophisticated—AI doesn't replace student thinking but scaffolds the discourse process while students do the knowledge building work. The proposed solution ensures AI provides options while students maintain choice and decision-making authority.

**CraftPad** was introduced as a design tool for teachers to plan KB activities, map student knowledge, and design interventions. Features include: flexible workspace with infinite canvas, context-aware KB coach (understands current KB community state, suggests scaffolds based on where the community is, adapts to teacher's goals), and rich opportunities for teacher decisions (AI provides options, teacher makes pedagogical choices, maintains teacher agency).

CraftPad exemplifies AI augmenting professional judgment rather than replacing it.

### Closing Keynote: Hung & Tan—Learning vs. Performance Augmentation

The closing keynote drew a CRITICAL distinction between **learning augmentation and performance augmentation**:

**Performance Augmentation**: Goal: Improved output (better products, faster completion)
- User role: Operator of AI tool—Example: Grammarly polishing writing—Outcome: Better product—Learning: Minimal or none

**Learning Augmentation**: Goal: Improved understanding and capability—User role: Active learner with AI support—Example: Using AI to explore concepts and receive feedback on thinking—Outcome: Better learner—Learning: Central purpose

The danger identified: Current AI use in education focuses heavily on performance augmentation, which doesn't build capability and may actually reduce learning through cognitive offload.

Examples contrasting the two approaches:

**Writing an essay**: Performance: AI writes, student submits—Learning: Student writes, AI questions the reasoning

**Solving math problems**: Performance: AI solves the problem—Learning: AI shows steps, student explains the reasoning

**Coding**: Performance: AI generates complete code—Learning: AI explains concepts, student writes code

**Research**: Performance: AI summarizes sources—Learning: AI helps student synthesize across sources

The key principle: In performance augmentation, AI does the work; in learning augmentation, the student does the work while AI supports thinking.

This framework echoed themes throughout the conference: Prof. Cukurova's distinction between replacement and amplification, Prof. Dawson's emphasis on assessing process not just product, and Knowledge Building's focus on epistemic agency.

The closing keynote tied together the conference's three-day progression: **Day 1: Learning ABOUT AI**—Understanding how AI works—**Day 2: Learning WITH AI**—Using AI as a tool—**Day 3: Learning BEYOND AI**—Using AI to augment learning without bypassing it

**"Learning Beyond AI"** was interpreted as using AI to augment learning rather than replace it—developing human capabilities that transcend what AI can do, preparing for more advanced AI while maintaining human agency, and building collective intelligence that goes beyond individual AI assistance.


[End of Part 1]

---

# Part 2: Personal Thoughts and Responses

These are my thoughts and responses to each individual talk/discussion, as I recall them

Opening remarks: Prof Liu Woon Chia (NIE)

Innovative AI solutions: proper and responsible use.
Ethical AI policies?

Keynote 5: Prof Mairead Pratschke
The new Contest: The Pedagogical Imperative
MIT Study: "Your Brain on ChatGPT"
- takeaway: learning activities should match learning outcomes

Industry response: add a tweak to how model already behaves, e.g. ChatGPT/Gemini guided learning mode to avoid giving answer
- This is not a pedagogical/androgogical model of learning

AI as presence: avatars, assistants, tutors, chatbots, agents, humanoid robots, digital humans

Reasoners: 'thinking machines'

chart AI development in 2 directions: intelligence, integration

The Memory Paradox: declarative -> procedural

cognitive function: storing key info is a pillar of it --> importance of context building

we need cognitive function to learn
skills = procedural knowledge + context

referenced Karpathy's Software 3.0

2025 - from prompt engineering to context engineering

Contextual Intelligence: Computational + pedagogical + content intelligence

Pedagogical Systems

  Context Engineering           LLM (CPU)
User  -- prompting    -->  |
Files -- retrieval    -->  |  Context Window
Tools -- tool calling -->  |

Contextual Framework:
- Cognitive
- Learning
- Pedagogical
- Technical

Learning from experience (in virtual worlds)

Agentic Community of Enquiry
- Cognitive agents
- pedagogical agents
- social agents

Keynote 6: Ptof Philip Dawson (Deakin University)

If AI can do the assessment task and pass, you have an assessment design problem

Students are using AI to:
- explain concepts
- summarise
- suggest research ideas

Unsupervised multiple choice is no longer reliable for summative assessment
ChatGPT Agent can already complete an online course mostly smoothly

Guiding principles:
- Assessment & learning (A&L) experiences equip students to participate ethically in an AI-ubiquitous society
- Forming trustworthy judgements about student learning requires multiple, inclusive, contextualised approaches to assessment

Ref: TEQSA - Assessment reform for the age of AI

Propositions:
1. Assessment should emphasise appropriate, authentic, engagement with AI
2. Emphasise systemic approach to program assessment aligned with disciplines/qualifications
3. Emphasise process of learning
4. Opportunities for students to work appropriately with each other and AI
5. Emphasise (assessment) security at meaningful points across a program to inform decisions about progression and completion

*What* we assess vs how we assess

Assessment validity matters more than cheating: don't hurt validity to block AI

Future-authentic assessment

Reverse Scaffolding
- allow use of genAI for outcomes students have sufficiently mastered

AI tools for production don't respect the Zome of Proximal Development (ZPD, Lev Vygotsky)

Cognitive scaffolding: let students offload busywork (extraneous, not intrinsic cognitive load)

Enable evaluative judgement (but not only that)

Discursive: instructions about use of AI e.g. can use for editing but not for writing

Structural: task requirements & structure that are unavoidable, e.g. "we will have a conversation with you about your work"

Prefer the latter

Panel discussion
- Prof Mairead Pratschke
- Prof Philip Dawson
- Mdm Lee Lin Yee (MOE) - Divisional Director of ETD
- Mr Jason See (MOE) - CIO of MOE

Moderated by Assoc. Prof. Teo Tang Wee

_The emotions of learning_

Qns
1. What does the new hybrid model mean for teachers and students?
2. Best practices internationally for use of AI in assessment
   - students already using AI for feedback
   - AI to do summative work?
   - what shoudl educators be able to do with AI?
3. AI can do a lot -- but should it?
   - Encourage schools to experiment, fail fast, fail forward (Mdm Lee)
   - Unify learning and assessment (instead of assessment being separate, summative)
   - Planning lessons faster -- but is the lesson better?
   - Tools for teachers
   - How to pick up signals about student progress?
   - Wellness tools: managing screen time, etc
4. Policy & tech are "seatbelts", help people feel safe (data privacy wise) so they can go fast

Context & infrastructure determines what we can and cannot do

As users, we are responsible for educating the tools/platforms we use

Tools work if used well; need to equip students & teachers with knowledge on how to use, how to learn (meta-learning)

Parallel talk (LT1)

Prof. Venky Shankuraraman - SMU Vice-Provost (Education)

Adapt -- Incorporate -- Detect approach to AI

Detect - last resort, SMU uses DRIVE approach, intended to be difficult
- intention: encourage faculty to adapt and incorporate

Challenges
- overreliance on AI
- curriculum relevance
- assessment validity
- equity and access
- ethical and societal literacy
- faculty readiness

Curriculum for AI
- critical thinking & judgement
- human-human and human-AI collaboration
- data literacy, digital fluency
- creativity & problem-solving
- ethics, policy, responsible use

Guiding Principles
- universality with depth
- ethics first
- applied & experimental
- interdisciplinary
- agility
- access & inclusion

Ideal curriculum structure
Yr 1: Foundation
Yr 2: Applied Skills
Yr 3: Strategic Integration
Yr 4: Capstone & Real Impact

Dr Ho Shen Yong (NTU) - Systemic Framework for Assessment & Curiculum Design in AI Era (on behalf of presenter)

Head of InSPIRe - Institute for Pedagogical Innovation, Research, and Excellence

Two-lane approach

Lane 1: assessment of learning

Lane 2; assessment for & as learning

SOLO: Structure of Observed Learning Outcomes

85% rule for optimal learning

assessment 2x2: unsupervised vs supervised, no AI vs AI assisted

unsupervised, AI assisted: unreliable, avoid; expect students to use AI here

Panel discussion

- "How do we teach critical thinking, what tangible outcome of critical thinking can we assess?"
- Data Analysis qns? Give additional data, see response
- Critical thinking is discipline-specific, depends on domain details

---

# EdTech Masterplan 2030

To begin this review with some context, the Ministry of Education unveiled its [EdTech Masterplan 2030](https://www.moe.gov.sg/education-in-sg/educational-technology-journey/edtech-masterplan). Not explicitly mentioned is a strong push towards AI integration in education. However this was formulated, in staff meetings this year I felt it as a strong directive to adopt generative AI (genAI) tools in teaching and learning, but with little actionable guidance around how to do so, and why.

The masterplan mentions 3 key enablers needed to enable implementation:

> Key enabler 1: Learning analytics and data
> Key enabler 2: EdTech infrastructure and solutions to support school processes and meet rapidly changing needs
> Key enabler 3: EdTech ecosystem

I've seen all three play out on [Student Learning Space (SLS)](https://www.moe.gov.sg/education-in-sg/student-learning-space), the national learning platform for Singapore schools. Its primary focus appears to be delivery of micro-lesson units for primary and seconday schools, with some AI features such as a feedback bot that provides feedback on simple responses. Suitability for higher education (K-12 and up) appears limited, in what little time I have spent exploring it.

It is through the lens of these three enablers that I will review the conference sessions I attended.

## Learning analytics and data

The learning analytics and data currently available through SLS primarily focuses on tracking student progress through lesson units, and response accuracy. In AIFE 2025, I saw many presentations exploring other forms of learning analytics, with many presenters recognising their limitations. They are going into process telemetry: looking at number of edits, rate of edits, pause times and duration, and other process-oriented metrics to better understand student learning not just as an outcome but *as a process*.

This level of telemetry enables more sophisticated analyses of the kind that usually requires an attentive teacher who has noticed for example that Bobby has been staring at the screen without typing for the past 5 minutes, or Tim has been going over the same paragraph multiple times without meaningful changes. It can detect confusion or anxiety as it happens / if it happened, regardless of the polish in the final product, and thus unlocks the possibility of real-time interventions, or a discussion on the socio-emotional experience the student underwent.

As presented, these are still cutting-edge research projects, not yet ready for wider deployment across various educational settings. The telemetry itself is not a new thing; online ad platforms have been doing this over a decade for optimising user engagement, but that capability does not yet exist in a way suitable for education.

If we want to give more meaningful, engaging feedback beyond the same kind of responses we already give on assignments, I can already do this for 1–3 students in a small group setting, but to do it for a class of 20 **I need these attention-scaling tools**.

## EdTech infrastructure and solutions

The "strong directive to use genAI tools in teaching and learning" I mentioned earlier manifests on the ground as frequent, strong reminders to explore the use of genAI tools in lesson planning, classroom activities, and assessments, but with little explanation of *why* or *how*. In practice that looks a lot like:

A: "Have you tried using ChatGPT to generate some quiz questions for your next lesson?"
B: "Yes, I uploaded some past year papers and prompted it to generate questions for <topic>."
A: "You uploaded *what*?"

or

A: "Have you tried using ChatGPT to improve student feedback?"
B: "Yes, I copy-paste their answers into ChatGPT and ask it to give feedback, and sometimes I also tell it more about the student so it gives more sensitive and empathetic feedback."
A: "You told it *what* about the student?"

While we are still undergoing trial periods with (limited versions of) Copilot and Gemini, which experientially feel like using ChatGPT with the same amount of copy-pasting involved, there are few other platforms or tools being offered to educators to meaningfully integrate genAI into their teaching practice. If I talk to 5 other educators, I hear about 5 different genAI tools being trialled; three different AI voice generators, two different AI video generators, five different summarizers, and even more custom chatbot platforms. Everyone is hunting not for the best tool or even the right tool, but mainly for a free tool; with any luck this tool remains free for the next 1–2 years before they sink and we scramble for another free tool (and what about all that data???), or the company finds product-market fit and starts charging money for it and we scramble for another free tool (and what about all that data???).

For daily drivers we're stuck with Pair Chat, a government-provided Claude/ChatGPT-like interface, and whatever AI features get integrated into SLS over time, as the only officially-sanctioned platforms for anything involving potentially sensitive student data.

Those are solutions, not infrastructure. Infrastructure would be a platform that allows educators to plug in different genAI models or tools as they see fit, and tools for building genAI-powered applications beyond chatbots and prompt pasting. Most importantly, a robust data governance framework with accompanying tools for handling student data securely and ethically, such as data anonymization, consent management, and audit trails. Without these, educators get laden with more checklists and cognitive load about what data can be used where, and whatever productivity gains we get are squandered away doing compliance work manually.

At the talks I attended, I heard no educators using existing platforms, only existing models. They have had to develop their own solutions, and they see infrastructure as the next question mark they have to tackle. Prof. Peter Johnson (Imperial College London) came to present [Lambda Feedback](https://github.com/lambda-feedback/evaluation-function-api), a platform for connecting AI micro-services to quiz platforms. He's looking for collaborators to work on a common API schema for the microservice, because one doesn't exist yet.

We can't wait for vendors or the edtech industry to build these for us. We have to build them ourselves, or at least co-develop them with vendors, because we know our needs best.

## EdTech ecosystem

Locally, the edtech ecosystem is still nascent. There are few local edtech startups, and even fewer focusing on AI in education. As interesting as some of these educational platforms and tools are, they often do not align with local educational goals, curricula, or cultural norms; reseachers cannot afford hosting these solutions for local schools, schools often don't have enough funding to pay for a vendor to implement these solutions, and even when they do, the solutions are often existing products with some adaptation. Without startups or local companies building solutions from the ground up for local needs, the ecosystem remains fragmented and underdeveloped.

It feels like a choice between building everything in-house, or adopting foreign solutions that may not fit well. There is little middle ground where local edtech companies can co-develop solutions with schools and researchers to create tools that are both effective and contextually relevant.

There is excellent work being done to build some sorely needed tools; I particularly enjoy and am inspired by [Chan Kuang Wen's extensive work](https://kwen1510.github.io/home/) building semantic search and mentoring students working in this area. But when I ask myself where I see EdTech in the next 5 years ... I think it's got to be more than a smarter search box, another chatbot with customisable prompts, or a more contextually aware quiz generator or feedback bot. As <check-citation>Dr Simon See</check-citation> put it, we need to move on from prompt engineering to context engineering. If we're not careful, instead of maggot therapy that eats away the non-human parts, we might end up with Frankenstein's monster, a patchwork of disconnected AI tools that don't quite work with each other, leaving the human to do the gluework of copy-pasting prompts, downloading and uploading documents, and sieving out personal data manually.

# EdTech and AI in the next 5 years

Prof. Peter Johnson of ICL is trying to collaborate on AI microservices; Dr Ho Jia Xuan is experimenting with telemetry tools for writing; more than a number of names are working on context-aware bots. We need more of these efforts, ideas, and projects; AI for learning and AI for production have different goals and therefore needs. In 10 minutes I came up with some missing pieces:
- UI components for building AI-infused apps
- SDKs for data handling and model selection
- Specs and schemas for data exchange and interoperability
- Evaluation and test suites for learning AI

The last and IMO most important piece is the platform: I can't yet imagine what a *universal* learning platform would look like, but I can imagine and ideate what might work for my college. More importantly, I need to pull together a team interested to explore, build, and test this platform; this work cannot and should not be done alone. If, as Dr Maira Pratschke puts it, "AI in learning is all about context, context, context", then *something* needs to be managing that context, and a cohesive context manager *is* going to end up being a platform for all of the above.
